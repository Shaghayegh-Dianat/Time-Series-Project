# -*- coding: utf-8 -*-
"""TS_main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NVkdX7glC5IrNlHhKxB9HXvuazD9SL8-
"""

import yfinance as yf
import pandas as pd
import numpy as np
from scipy.stats import jarque_bera
from statsmodels.stats.diagnostic import acorr_ljungbox
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller, acf, pacf
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from scipy.stats import chi2
from arch import arch_model
from statsmodels.stats.diagnostic import het_arch
from arch import arch_model
from sklearn.metrics import mean_squared_error, mean_absolute_error

data = yf.download("DOGE-USD", start="2021-12-01", end="2023-11-01")
price=data['Close']
print('price',price)
price.plot(title='DOGE price')
returns = 100 * price.pct_change().dropna()
diff = price.diff() .dropna()
diff.plot(title="DOGE differenced price")

pip install arch

returns.plot(title="DOGE  percentage return")

# Calculate statistical parameters
mean = price.mean()
median = price.median()
maximum = price.max()
minimum = price.min()
std_dev = price.std()
skewness = price.skew()
kurtosis = price.kurtosis()

# Perform the Jarque-Bera test
jb_stat, jb_p_value = jarque_bera(price)

# Sum and sum of squared deviations
sum_price = price.sum()
sum_sq_dev = ((price - mean) ** 2).sum()

# Number of observations
observations = len(price)

# Create a DataFrame for the table
table = pd.DataFrame({
    'Statistic': [
        'Mean', 'Median', 'Maximum', 'Minimum', 'Std. dev.',
        'Skewness', 'Kurtosis', 'Jarque-Bera', 'Probability',
        'Sum', 'Sum Sq. dev.', 'Observations'
    ],
    'Price': [
        mean, median, maximum, minimum, std_dev,
        skewness, kurtosis, jb_stat, jb_p_value,
        sum_price, sum_sq_dev, observations
    ]
})

print(table.to_string(index=False))

# Calculate statistical parameters
mean = returns.mean()
median = returns.median()
maximum = returns.max()
minimum = returns.min()
std_dev = returns.std()
skewness = returns.skew()
kurtosis = returns.kurtosis()

# Perform the Jarque-Bera test
jb_stat, jb_p_value = jarque_bera(returns)

# Sum and sum of squared deviations
sum_returns = returns.sum()
sum_sq_dev = ((returns - mean) ** 2).sum()

# Number of observations
observations = len(returns)

# Create a DataFrame for the table
table = pd.DataFrame({
    'Statistic': [
        'Mean', 'Median', 'Maximum', 'Minimum', 'Std. dev.',
        'Skewness', 'Kurtosis', 'Jarque-Bera', 'Probability',
        'Sum', 'Sum Sq. dev.', 'Observations'
    ],
    'returns': [
        mean, median, maximum, minimum, std_dev,
        skewness, kurtosis, jb_stat, jb_p_value,
        sum_returns, sum_sq_dev, observations
    ]
})

# Display a clean table
print(table.to_string(index=False))

"""check stationary :PRICE has a unit root"""

adf_test = adfuller(price)

results_table = {
    "ADF Statistic of prices": adf_test[0],
    "p-value": adf_test[1].round(3),
    "1% Critical Value": adf_test[4]['1%'],
    "5% Critical Value": adf_test[4]['5%'],
    "10% Critical Value": adf_test[4]['10%']
}

for key, value in results_table.items():
    print(f"{key}: {value}")
p_value= adf_test[1].round(3)
if p_value > 0.05:
    print("Series is non-stationary so apply differencing ( or use returns).")
    series = returns
else:
    print("Series is stationary.")
    series = price

adf_test = adfuller(returns)

# Display the results
results_table = {
    "ADF Statistic of returns": adf_test[0],
    "p-value": adf_test[1].round(3),
    "1% Critical Value": adf_test[4]['1%'],
    "5% Critical Value": adf_test[4]['5%'],
    "10% Critical Value": adf_test[4]['10%']
}

for key, value in results_table.items():
    print(f"{key}: {value}")

nlags = 35
# Compute ACF and PACF
acf_values, confint = acf(series, nlags=nlags, alpha=0.05, fft=False, qstat=False)
pacf_values = pacf(series, nlags=nlags, method='ywm')

# Ljung-Box test for Q-statistics and p-values
ljungbox_results = acorr_ljungbox(series, lags=nlags, return_df=True)

# Create DataFrame (mimic Table 4)
table = pd.DataFrame({
    'Lags(k)': range(1, nlags+1),
    'AC': acf_values[1:nlags+1],
    'PAC': pacf_values[1:nlags+1],
    'Q-stat': ljungbox_results['lb_stat'],
    'Prob': ljungbox_results['lb_pvalue']
})

# Round values for cleaner output
table = table.round(4)


print("\ACF/PACF Table:")
print(table.to_string(index=False))

# Plot ACF and PACF
plot_acf(series, lags=nlags, title='ACF of Doge return')
plot_pacf(series, lags=nlags, title='PACF of Doge return', method='ywm')
plt.show()

# Define parameter combinations
p_values = [3,8,10,14,15,16]
q_values = [3,8,10,14,15,16]
results = []

# Fit models and calculate AIC
for p in p_values:
    for q in q_values:
        try:
            model = ARIMA(series, order=(p, 0, q))
            result = model.fit()
            results.append({'p': p, 'q': q, 'AIC': round(result.aic, 3)})
        except:
            results.append({'p': p, 'q': q, 'AIC': None})

# Create sorted results table
results_df = pd.DataFrame(results)
results_table = results_df.sort_values('AIC').reset_index(drop=True)

print("ARIMA Model Comparison (d=1):")
print(results_table.to_string(index=False))

# Define parameter combinations
p_values = [2,3,8,14,15,16]
q_values = [2,3,8,10,14,15,16]
results = []

# Fit models and calculate AICC
for p in p_values:
    for q in q_values:
        try:
            model = ARIMA(series, order=(p, 0, q))
            result = model.fit()
            results.append({'p': p, 'q': q, 'AICC': round(result.aicc, 3)})
        except:
            results.append({'p': p, 'q': q, 'AICC': None})

# Create sorted results table
results_df = pd.DataFrame(results)
results_table = results_df.sort_values('AICC').reset_index(drop=True)

print("ARIMA Model Comparison (d=1):")
print(results_table.to_string(index=False))

# Fit ARIMA(16,1,15)
model = ARIMA(returns, order=(16,0, 15), trend='c')
arima_result = model.fit()

# Extract parameters
params = arima_result.params
stderr = arima_result.bse
tstats = arima_result.tvalues
pvalues = arima_result.pvalues
sigmasq = np.var(arima_result.resid)

# Model fit statistics
AIC = arima_result.aic
BIC= arima_result.bic
log_likelihood = arima_result.llf
print("AIC ARIMA(16,1,15):",AIC)
print("BIC ARIMA(16,1,15):",BIC)
print("log_likelihood:",log_likelihood)

residuals = arima_result.resid.dropna()
nlags =25
# Perform ARCH-LM test

lm_stat, lm_pval, F_stat, F_pval = het_arch(residuals, nlags=nlags)

# Create results table
results = pd.DataFrame({
    'Statistic': ['ARCH-LM Test Statistic', 'LM p-value', 'F-statistic', 'F p-value'],
    'Value': [lm_stat, lm_pval, F_stat, F_pval]
}).round(4)


print("ARCH Test Results")
print(results.to_string(index=False))

# Interpretation: Check p-values
if lm_pval > 0.05:
    print("\nFail to reject the null hypothesis: No significant ARCH effects.")
else:
    print("\nReject the null hypothesis: Significant ARCH effects detected.")

# Fit GARCH(1,1) on residuals
model_garch = arch_model(residuals, vol='GARCH',dist='skewt', p=1, q=1)
result_garch = model_garch.fit()


print("ARIMA(16,1,15) Coefficients:")
print(params.round(3))

print("\nGARCH(1,1) Coefficients:")
print(result_garch.params.round(3))

def garch_se(garch_result):
    # Extract the covariance matrix of the parameters
    covariance_matrix = garch_result.param_cov

    # Calculate standard errors as the square root of the diagonal elements
    std_errors = np.sqrt(np.diag(covariance_matrix))

    # Create a pandas Series with parameter names for better readability
    std_errors_series = pd.Series(std_errors, index=garch_result.params.index)

    return std_errors_series

# Extract all ARIMA coefficients and their statistics
arima_params = arima_result.params
arima_se = arima_result.bse
arima_z = arima_result.tvalues
arima_p = arima_result.pvalues

# Extract all GARCH coefficients and their statistics
garch_params = result_garch.params
garch_se = garch_se(result_garch)
garch_z = result_garch.tvalues
garch_p = result_garch.pvalues

# Combine ARIMA and GARCH results into a single table
data = {
    'Variable': list(arima_params.index) + list(garch_params.index),
    'Coefficient': list(arima_params.values) + list(garch_params.values),
    'Std. Error': list(arima_se.values) + list(garch_se.values),
    'z-statistic': list(arima_z.values) + list(garch_z.values),
    'Prob.': list(arima_p.values) + list(garch_p.values),
}

# Create the DataFrame
results_table = pd.DataFrame(data)

# Round values for better readability
results_table[['Coefficient', 'Std. Error', 'z-statistic', 'Prob.']] = results_table[
    ['Coefficient', 'Std. Error', 'z-statistic', 'Prob.']
].round(4)

# Display the full results table
print(results_table)

# Model fit statistics
AIC = result_garch.aic
BIC= result_garch.bic
aic=arima_result.aic
bic=arima_result.bic
print("AIC ARIMA(16,1,15):", aic)
print("BIC ARIMA(16,1,15):",bic)
print("AIC ARIMA(16,1,15)-GARCH(1,1):",AIC)
print("BIC ARIMA(16,1,15)-GARCH(1,1) :",BIC)

pip install arch

# Calculate residuals
main_residual = result_garch.resid.dropna()

# Square the residuals
squared_residual = main_residual ** 2

# Calculate ACF and PACF values
lag_acf = acf(main_residual, nlags=35)
lag_pacf = pacf(main_residual, nlags=35)

# Calculate Q-statistics and p-values using the Ljung-Box test
ljungbox_results = acorr_ljungbox(main_residual, lags=35, return_df=True)

# Create the lag table
lag_table = pd.DataFrame({
    "Lag": np.arange(1, 36),
    "AC": lag_acf[1:],
    "PAC": lag_pacf[1:],
    "Q-stat": ljungbox_results['lb_stat'].values,
    "Prob": ljungbox_results['lb_pvalue'].values
})

print("ACF/PACF Table residual:")
print(lag_table)

# Plot ACF/PACF
plot_acf(main_residual, lags=nlags, title='ACF of redidual of the ARIMA(16,1,15)-GARCH(1,1)',alpha=0.05)
plot_pacf(main_residual, lags=nlags, title='PACF of redidual of the ARIMA(16,1,15)-GARCH(1,1)', method='ywm')
plt.show()

# Ljung-Box test on squared standardized residuals
lb_test = acorr_ljungbox(main_residual, lags=[35], return_df=True)
print(f" Ljung-Box p-value: {lb_test['lb_pvalue'].values[0]:.4f}")

# Perform ARCH-LM test

lm_stat, lm_pval, F_stat, F_pval = het_arch(squared_residual, nlags=30)

# Create results table
results = pd.DataFrame({
    'Statistic': ['ARCH-LM Test Statistic', 'LM p-value', 'F-statistic', 'F p-value'],
    'Value': [lm_stat, lm_pval, F_stat, F_pval]
}).round(4)


print("ARCH Test Results")
print(results.to_string(index=False))

# Interpretation: Check p-values
if lm_pval > 0.05:
    print("\nFail to reject the null hypothesis: No significant ARCH effects.")
else:
    print("\nReject the null hypothesis: Significant ARCH effects detected.")

pip install arch

"""compare forecasted prices with real **prices**"""

data = yf.download("DOGE-USD", start="2021-12-01", end="2023-11-01")
price = data['Close'].dropna()

# Convert prices to log returns for better stationarity
log_returns = np.log(price / price.shift(1)).dropna()

#Import test data
real_prices = yf.download("DOGE-USD", start="2023-11-17", end="2023-12-02")['Close']
real_log_returns = np.log(real_prices / real_prices.shift(1)).dropna()


#Fit ARIMA Model
model_arima = ARIMA(log_returns, order=(16, 0, 15))
arima_result = model_arima.fit()

#Forecast ARIMA Mean (Log Returns)
forecast_steps = len(real_log_returns)
arima_forecast = arima_result.forecast(steps=forecast_steps).values.flatten()


if len(arima_forecast) != forecast_steps:
    arima_forecast = np.resize(arima_forecast, forecast_steps)

# Compute Future Log Returns
predicted_log_returns = arima_forecast

# Ensure predicted_log_returns has the correct shape
if len(predicted_log_returns) != forecast_steps:
    predicted_log_returns = np.resize(predicted_log_returns, forecast_steps)


cumulative_log_returns = np.cumsum(predicted_log_returns)

# Convert log returns back to prices
last_price = price.iloc[-1].item()
predicted_prices = [last_price * np.exp(ret) for ret in cumulative_log_returns]

#Create DataFrame with Forecasted Prices
forecast_df = pd.DataFrame({
    "Date": pd.date_range(start="2023-11-18", periods=forecast_steps, freq="D"),
    "Predicted Price": predicted_prices
})

print(forecast_df)


plt.figure(figsize=(10, 5))
plt.plot(forecast_df["Date"], forecast_df["Predicted Price"]*100,marker="s",linestyle="--", label="Forecasted Price", color="red" )
plt.plot(real_prices.index, real_prices.values*100, marker="o", linestyle="-", label="Actual Price",color='blue' )
plt.xlabel("Date")
plt.ylabel("Dogecoin Price (cent)")
plt.title("DOGE Price Forecast using ARIMA")
plt.legend()
plt.ylim(0, 25)
plt.grid()
plt.show()
print(real_prices)
#Evaluate Model Accuracy (RMSE, MAE, MAPE)
def calculate_mape(actual, predicted):
    return np.mean(np.abs((actual - predicted) / actual)) * 100

mape = calculate_mape(real_prices[1:].values, predicted_prices)
rmse = np.sqrt(mean_squared_error(real_prices[1:].values, predicted_prices))
mae = mean_absolute_error(real_prices[1:].values, predicted_prices)

print(f"RMSE: {rmse:.5f}")
print(f"MAE: {mae:.5f}")
print(f"MAPE: %{mape:.5f}")

residuals = arima_result.resid.dropna()

# Fit GARCH(1,1) on ARIMA residuals
model_garch = arch_model(residuals, vol='GARCH', dist='skewt', p=1, q=1)
result_garch = model_garch.fit(disp="off")

#Forecast ARIMA Mean (Log Returns)
forecast_steps = len(real_log_returns)
arima_forecast = arima_result.forecast(steps=forecast_steps).values.flatten()

#Forecast GARCH Volatility
garch_forecast = result_garch.forecast(horizon=forecast_steps)
garch_variance = garch_forecast.variance.values[-1, :]
garch_volatility = np.sqrt(garch_variance)

if len(arima_forecast) != forecast_steps:
    arima_forecast = np.resize(arima_forecast, forecast_steps)

if len(garch_volatility) != forecast_steps:
    garch_volatility = np.resize(garch_volatility, forecast_steps)

np.random.seed(42)
random_shocks = np.random.normal(0, 0.1, forecast_steps)

# Compute Future Log Returns
predicted_log_returns = arima_forecast + garch_volatility * random_shocks

# Ensure predicted_log_returns has the correct shape
if len(predicted_log_returns) != forecast_steps:
    predicted_log_returns = np.resize(predicted_log_returns, forecast_steps)


cumulative_log_returns = np.cumsum(predicted_log_returns)

# Convert log returns back to prices
last_price = price.iloc[-1].item()
predicted_prices = [last_price * np.exp(ret) for ret in cumulative_log_returns]

#Create DataFrame with Forecasted Prices
forecast_df = pd.DataFrame({
    "Date": pd.date_range(start="2023-11-18", periods=forecast_steps, freq="D"),
    "Predicted Price": predicted_prices
})

print(forecast_df)


plt.figure(figsize=(10, 5))
plt.plot(forecast_df["Date"], forecast_df["Predicted Price"]*100,marker="s",linestyle="--", label="Forecasted Price", color="red" )
plt.plot(real_prices.index, real_prices.values*100, marker="o", linestyle="-", label="Actual Price",color='blue' )
plt.xlabel("Date")
plt.ylabel("Dogecoin Price (cent)")
plt.title("DOGE Price Forecast using ARIMA-GARCH")
plt.legend()
plt.ylim(0, 25)
plt.grid()
plt.show()
print(real_prices)
#Evaluate Model Accuracy (RMSE, MAE, MAPE)
def calculate_mape(actual, predicted):
    return np.mean(np.abs((actual - predicted) / actual)) * 100

mape = calculate_mape(real_prices[1:].values, predicted_prices)
rmse = np.sqrt(mean_squared_error(real_prices[1:].values, predicted_prices))
mae = mean_absolute_error(real_prices[1:].values, predicted_prices)

print(f"RMSE: {rmse:.5f}")
print(f"MAE: {mae:.5f}")
print(f"MAPE: %{mape:.5f}")